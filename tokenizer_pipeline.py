import re
import os
from tokenizers import ByteLevelBPETokenizer
import json

# -----------------------------
# –ù–∞—Å—Ç—Ä–æ–π–∫–∏
# -----------------------------
CORPUS_FILE = "data/corpus.txt"        # –∏—Å—Ö–æ–¥–Ω—ã–π —Ç–µ–∫—Å—Ç (—Ç–≤–æ–π 4GB –∫–æ—Ä–ø—É—Å)
SAVE_DIR = "data"                       # –∫—É–¥–∞ —Å–æ—Ö—Ä–∞–Ω—è—Ç—å —Ñ–∞–π–ª—ã
VOCAB_SIZE = 15000                      # —Å–ª–æ–≤–∞—Ä—å —Ç–æ–∫–µ–Ω–æ–≤
MIN_FREQ = 2                            # –º–∏–Ω–∏–º–∞–ª—å–Ω–∞—è —á–∞—Å—Ç–æ—Ç–∞ —Ç–æ–∫–µ–Ω–∞

os.makedirs(SAVE_DIR, exist_ok=True)

# -----------------------------
# –®–∞–≥ 1: –ß–∏—Å—Ç–∫–∞ —Ç–µ–∫—Å—Ç–∞
# -----------------------------
def clean_text(text: str) -> str:
    text = re.sub(r"<[^>]+>", " ", text)                   # —É–±–∏—Ä–∞–µ–º HTML
    text = re.sub(r"\[[0-9]+\]", " ", text)               # —Ü–∏—Ñ—Ä—ã –≤ —Å–∫–æ–±–∫–∞—Ö
    text = re.sub(r"[^–∞-—è–ê-–Ø—ë–Åa-zA-Z”ô—ñ“£“ì“Ø“±“õ”©“ª”ò–Ü“¢“í“Æ“∞“ö”®“∫\s.,!?-]", " ", text)  # –º—É—Å–æ—Ä
    text = re.sub(r"\s+", " ", text)
    return text.strip()

print("üîπ –ß–∏—Å—Ç–∏–º –∫–æ—Ä–ø—É—Å...")
with open(CORPUS_FILE, "r", encoding="utf-8", errors="ignore") as f:
    raw_text = f.read()

clean = clean_text(raw_text)

clean_path = os.path.join(SAVE_DIR, "clean_corpus.txt")
with open(clean_path, "w", encoding="utf-8") as f:
    f.write(clean)
print(f"‚úÖ –ß–∏—Å—Ç—ã–π –∫–æ—Ä–ø—É—Å —Å–æ—Ö—Ä–∞–Ω—ë–Ω –≤ {clean_path}")

# -----------------------------
# –®–∞–≥ 2: –û–±—É—á–µ–Ω–∏–µ BPE-—Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞
# -----------------------------
print("üîπ –û–±—É—á–∞–µ–º BPE-—Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä...")
tokenizer = ByteLevelBPETokenizer()
tokenizer.train(
    files=[clean_path],
    vocab_size=VOCAB_SIZE,
    min_frequency=MIN_FREQ,
    special_tokens=["<s>", "<pad>", "</s>", "<unk>", "<mask>"]
)
tokenizer.save_model(SAVE_DIR, f"kazakh_bpe_{VOCAB_SIZE}")
print(f"‚úÖ –¢–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä –æ–±—É—á–µ–Ω –∏ —Å–æ—Ö—Ä–∞–Ω—ë–Ω –≤ {SAVE_DIR}/kazakh_bpe_{VOCAB_SIZE}-*")

# -----------------------------
# –®–∞–≥ 3: –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º —Ç–µ–∫—Å—Ç –≤ —Ç–æ–∫–µ–Ω—ã
# -----------------------------
print("üîπ –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º —Ç–µ–∫—Å—Ç –≤ —Ç–æ–∫–µ–Ω—ã...")
tokenizer.enable_truncation(max_length=256)  # –æ–±—Ä–µ–∑–∞–µ–º –¥–æ 256 —Ç–æ–∫–µ–Ω–æ–≤ –¥–ª—è GPT-0.5

token_ids = []
with open(clean_path, "r", encoding="utf-8") as f:
    for line in f:
        ids = tokenizer.encode(line).ids
        token_ids.extend(ids)

# –°–æ—Ö—Ä–∞–Ω—è–µ–º –º–∞—Å—Å–∏–≤ —Ç–æ–∫–µ–Ω–æ–≤
tokens_path = os.path.join(SAVE_DIR, "tokens.json")
with open(tokens_path, "w", encoding="utf-8") as f:
    json.dump(token_ids, f)

print(f"‚úÖ –ú–∞—Å—Å–∏–≤ —Ç–æ–∫–µ–Ω–æ–≤ —Å–æ—Ö—Ä–∞–Ω—ë–Ω –≤ {tokens_path}")
print(f"–û–±—â–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç–æ–∫–µ–Ω–æ–≤: {len(token_ids)}")